{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Testing\n",
    "\n",
    "All models are tested on the test data. The test data contains the last year of the time series for each of the cities.\n",
    "All Error metrics are calculated independently for \n",
    "\n",
    "- A) Frequency (Forecasting Horizon) ($f$)\n",
    "- B) City ($c$)\n",
    "\n",
    "The result are the distributions of the conditional errors regarding the objective vector $X$.\n",
    "\n",
    "- $\\textbf{E}(X | c, f)$\n",
    "- $\\textbf{E}(x_i | c, f) \\ \\forall i \\in X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX as ARIMA\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from copy import deepcopy as copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.style.use(\"seaborn\");plt.style.use(\"classic\")\n",
    "from itertools import product\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_outliers(df, q):\n",
    "    df2 = df.copy()\n",
    "    extreme_outliers = (df2 > df.quantile(q))\n",
    "    df2[extreme_outliers] = np.nan\n",
    "    df2.fillna(method=\"ffill\", inplace=True)  \n",
    "    return df2\n",
    "\n",
    "def detrend(data):\n",
    "    X = [i for i in range(0, len(data))]\n",
    "    X = np.reshape(X, (len(X), 1))\n",
    "    y = data.values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    trend = model.predict(X)\n",
    "    detrended = [y[i]-trend[i] for i in range(0, len(data))]\n",
    "    return pd.DataFrame(detrended, columns = data.columns, index=data.index)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "with open(\"data_w\", \"rb\") as f:\n",
    "    data_w = pickle.load(f)\n",
    "    \n",
    "with open(\"data_2w\", \"rb\") as f:\n",
    "    data_2w = pickle.load(f)\n",
    "    \n",
    "with open(\"data_3w\", \"rb\") as f:\n",
    "    data_3w = pickle.load(f)\n",
    "    \n",
    "with open(\"data_m\", \"rb\") as f:\n",
    "    data_m = pickle.load(f)\n",
    "    \n",
    "cities = list(data_m.keys())\n",
    "\n",
    "#Important Analysis parameters\n",
    "starting_year = \"2008\"\n",
    "target_vector = ['create', 'modify','tag_add',\n",
    "            'tag_del', 'tag_change','loc_change',\n",
    "            'new_mapper']\n",
    "outlier_threshold = .99\n",
    "detrend_data=False\n",
    "\n",
    "for dat in [data_w, data_2w, data_3w, data_m]:\n",
    "    for c in dat:\n",
    "        dat[c] = impute_outliers(dat[c][target_vector][starting_year:], outlier_threshold).dropna()\n",
    "        if detrend_data:\n",
    "            dat[c] = detrend(dat[c])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_ARIMA(data, order=(2,1,1)):\n",
    "    pred_vec = list()\n",
    "    for col in data.columns:\n",
    "        series = data[col].copy()\n",
    "        model = ARIMA(series, order=order)\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        pred = output[0]\n",
    "        pred_vec.append(pred)\n",
    "    return np.array(pred_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expo_weights(T,alpha):\n",
    "    t = np.array(list(range(1,T+1)))\n",
    "    return np.flip(((1-alpha)*alpha**t / (1-alpha)**T)/((1-alpha)*alpha**t / (1-alpha)**T).sum())\n",
    "def linear_weights(T):\n",
    "    t = np.array(list(range(1,T+1)))\n",
    "    return t / t.sum()\n",
    "\n",
    "def pred_WMA(data, window, test_size, weights,alpha=None):\n",
    "    preds, index = list(), list()\n",
    "    df = data[-test_size-window:].copy()\n",
    "    df_test = data[-test_size:].copy()\n",
    "    for i in range(window, len(df)):\n",
    "        now = df.iloc[i]\n",
    "        prev = df.iloc[i-window:i]\n",
    "        if weights == \"exponential\":\n",
    "            prediction = prev.apply(lambda x : x*expo_weights(window, alpha)).sum()\n",
    "        elif weights == \"linear\":\n",
    "            prediction = prev.apply(lambda x : x*linear_weights(window)).sum()\n",
    "        else:\n",
    "            raise Exception(\"Unknown weight method.\")\n",
    "        date = df.index[i]\n",
    "        preds.append(prediction)\n",
    "        index.append(date)\n",
    "    return pd.DataFrame(preds, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function aggregates the POI data, adds city dummies and shuffles the data.\n",
    "# Then a model is fitted by using all of the data except for the testing periods\n",
    "# Returns the Model\n",
    "def global_training(frequency,model,max_lag, city_dummies):\n",
    "    \n",
    "    if frequency == \"1w\":\n",
    "        data = copy(data_w)\n",
    "        test_size = 52\n",
    "    elif frequency == \"2w\":\n",
    "        data = copy(data_2w)\n",
    "        test_size = 26\n",
    "    elif frequency == \"3w\":\n",
    "        test_size = 17\n",
    "        data = copy(data_3w)\n",
    "    elif frequency == \"4w\":\n",
    "        data = copy(data_m)\n",
    "        test_size = 12\n",
    "    cols = ['create', 'modify','tag_add',\n",
    "            'tag_del', 'tag_change','loc_change',\n",
    "            'new_mapper']\n",
    "\n",
    "    dfs = list()\n",
    "    for c in data: \n",
    "        df = data[c].copy()\n",
    "        if model == \"RF\":\n",
    "            #add additional temporal features for the RF\n",
    "            for col in pd.get_dummies(df.index.month, prefix=\"month\").columns:\n",
    "                df[col] = pd.get_dummies(df.index.month, prefix=\"month\")[col].values\n",
    "            df[\"week_of_year\"] = df.index.week\n",
    "            df[\"day_of_month\"] = df.index.day\n",
    "            if city_dummies:\n",
    "                df[\"city\"] = [c for _ in range(len(df))]\n",
    "        elif model == \"VAR\":\n",
    "            df = df.diff().dropna()\n",
    "        for col in cols:\n",
    "            for l in range(1, max_lag+1):\n",
    "                df[col+\"(t-%s)\" % l] = df[col].shift(l)\n",
    "        df.reset_index(inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df = df[:-test_size]\n",
    "        dfs.append(df)\n",
    "\n",
    "    res = dfs[0].append(dfs[1])\n",
    "    for frame in dfs[2:]:\n",
    "        res = res.append(frame)\n",
    "    del res[\"index\"]\n",
    "    res = res.reset_index()\n",
    "    del res[\"index\"]\n",
    "    if city_dummies:\n",
    "        encoded = pd.get_dummies(res[\"city\"])\n",
    "        del res[\"city\"]\n",
    "        res = res.join(encoded)\n",
    "        \n",
    "    #shuffle\n",
    "    res = res.sample(frac=1).reset_index(drop=True)\n",
    "    Y = res[cols]\n",
    "    X = res[res.columns.difference(cols)]\n",
    "    #print(X.shape, Y.shape)\n",
    "    if model == \"RF\":\n",
    "        Model = RandomForestRegressor(n_estimators=100,\n",
    "                                       max_depth = None, n_jobs=-1,\n",
    "                                        max_features = int(X.shape[1] / 3.))\n",
    "    elif model == \"VAR\":\n",
    "        Model = LinearRegression(n_jobs=-1)\n",
    "        \n",
    "    Model.fit(X, Y)\n",
    "    \n",
    "    return Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the results into a large dataframe in order to do analysis of the conditional performance of the models\n",
    "# This is a trivial task and just makes plotting easier\n",
    "def aggregate(res_w, res_2w, res_3w, res_4w):\n",
    "    city, frequency, target, error, model = [], [], [], [], []\n",
    "    targets = [\"create\", \"modify\", \"tag_add\", \"tag_del\", \"tag_change\", \"loc_change\", \"new_mapper\"]\n",
    "    for c in res_w:\n",
    "        model_results = res_w[c]\n",
    "        for var in targets:\n",
    "            for i in range(len(model_results[var])):\n",
    "                error.append(model_results[var].values[i])\n",
    "                model.append(model_results[var].index[i])\n",
    "                target.append(var)\n",
    "                city.append(c)   \n",
    "                frequency.append(\"1w\")\n",
    "    for c in res_2w:\n",
    "        model_results = res_2w[c]\n",
    "        for var in targets:     \n",
    "            for i in range(len(model_results[var])):\n",
    "                error.append(model_results[var].values[i])\n",
    "                model.append(model_results[var].index[i])\n",
    "                target.append(var)\n",
    "                city.append(c)   \n",
    "                frequency.append(\"2w\")\n",
    "    for c in res_3w:\n",
    "        model_results = res_3w[c]\n",
    "        for var in targets:\n",
    "            for i in range(len(model_results[var])):\n",
    "                error.append(model_results[var].values[i])\n",
    "                model.append(model_results[var].index[i])\n",
    "                target.append(var)\n",
    "                city.append(c)   \n",
    "                frequency.append(\"3w\")\n",
    "    for c in res_4w:\n",
    "        model_results = res_4w[c]\n",
    "        for var in targets:\n",
    "            for i in range(len(model_results[var])):\n",
    "                error.append(model_results[var].values[i])\n",
    "                model.append(model_results[var].index[i])\n",
    "                target.append(var)\n",
    "                city.append(c)   \n",
    "                frequency.append(\"4w\")\n",
    "                \n",
    "    return pd.DataFrame({\"City\" : city,\n",
    "                         \"Target\" : target, \"Error\" :error,\n",
    "                        \"Frequency\" : frequency, \"Model\" : model})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last *\n",
    "def rolling_predictions(frequency, city):\n",
    "    if frequency == \"1w\":\n",
    "        data = data_w[city].copy()\n",
    "        test_size = 52 \n",
    "        SMA_window = 10\n",
    "        EWMA_alpha, EWMA_window = .5, 15\n",
    "        LWMA_window = 15\n",
    "        max_lag_rf, max_lag_var = 4, 1\n",
    "        max_lag_grf, max_lag_grf_d, max_lag_gvar = 4, 4, 2\n",
    "    elif frequency == \"2w\":\n",
    "        data = data_2w[city].copy()\n",
    "        test_size = 26\n",
    "        SMA_window = 6\n",
    "        EWMA_alpha, EWMA_window = .5, 15\n",
    "        LWMA_window = 10\n",
    "        max_lag_grf, max_lag_grf_d, max_lag_gvar = 4, 4, 2\n",
    "        max_lag_rf, max_lag_var = 4, 1\n",
    "    elif frequency == \"3w\":\n",
    "        test_size = 17\n",
    "        data = data_3w[city].copy()\n",
    "        SMA_window = 4\n",
    "        EWMA_alpha, EWMA_window = .5, 15\n",
    "        LWMA_window = 10\n",
    "        max_lag_grf, max_lag_grf_d, max_lag_gvar = 4, 4, 2\n",
    "        max_lag_rf, max_lag_var = 3, 1\n",
    "    elif frequency == \"4w\":\n",
    "        data = data_m[city].copy()\n",
    "        test_size = 12\n",
    "        SMA_window = 4\n",
    "        EWMA_alpha, EWMA_window = .5, 15\n",
    "        LWMA_window = 6\n",
    "        max_lag_rf, max_lag_var = 2, 1\n",
    "        max_lag_grf, max_lag_grf_d, max_lag_gvar = 2, 2, 2\n",
    "\n",
    "    cols = target_vector\n",
    "    data_rf = data.copy()\n",
    "    data_grf = data.copy()\n",
    "    data_grf_d = data.copy()\n",
    "    data_orig = data.copy()\n",
    "    data_var = data.copy().diff().dropna()\n",
    "    data_gvar = data_var.copy()\n",
    "\n",
    "    pred_BL1 = data_orig[-test_size-1:].shift(1).dropna().values\n",
    "    pred_SMA = data_orig[-test_size-SMA_window:].shift(1).rolling(SMA_window).mean().dropna().values\n",
    "    pred_EWMA = pred_WMA(data_orig, EWMA_window, test_size, weights=\"exponential\",alpha=EWMA_alpha)\n",
    "    pred_LWMA = pred_WMA(data_orig, LWMA_window, test_size, weights=\"linear\",alpha=None)\n",
    "    \n",
    "    grf_dummy = global_training(frequency, \"RF\",max_lag_grf_d, city_dummies=True)\n",
    "    grf = global_training(frequency, \"RF\",max_lag_grf, city_dummies=False)\n",
    "    gvar = global_training(frequency, \"VAR\",max_lag_gvar, city_dummies=False)\n",
    "\n",
    "    for col in data_rf:\n",
    "        for l in range(1, max_lag_rf+1):\n",
    "            data_rf[col+\"(t-%s)\" % l] = data_rf[col].shift(l)\n",
    "    data_rf.dropna(inplace=True)\n",
    "    \n",
    "    for col in data_grf:\n",
    "        for l in range(1, max_lag_grf+1):\n",
    "            data_grf[col+\"(t-%s)\" % l] = data_grf[col].shift(l)\n",
    "    data_grf.dropna(inplace=True)\n",
    "\n",
    "    for col in data_grf_d:\n",
    "        for l in range(1, max_lag_grf_d+1):\n",
    "            data_grf_d[col+\"(t-%s)\" % l] = data_grf_d[col].shift(l)\n",
    "    data_grf_d.dropna(inplace=True)\n",
    "\n",
    "    for col in data_var:\n",
    "        for l in range(1, max_lag_var+1):\n",
    "            data_var[col+\"(t-%s)\" % l] = data_var[col].shift(l)\n",
    "    data_var.dropna(inplace=True)\n",
    "\n",
    "    for col in data_gvar:\n",
    "        for l in range(1, max_lag_gvar+1):\n",
    "            data_gvar[col+\"(t-%s)\" % l] = data_gvar[col].shift(l)\n",
    "    data_gvar.dropna(inplace=True)\n",
    "\n",
    "    #add additional temporal features for the RF\n",
    "    for col in pd.get_dummies(data_rf.index.month, prefix=\"month\").columns:\n",
    "        data_rf[col] = pd.get_dummies(data_rf.index.month, prefix=\"month\")[col].values\n",
    "    data_rf[\"week_of_year\"] = data_rf.index.week\n",
    "    data_rf[\"day_of_month\"] = data_rf.index.day\n",
    "\n",
    "    for col in pd.get_dummies(data_grf.index.month, prefix=\"month\").columns:\n",
    "        data_grf[col] = pd.get_dummies(data_grf.index.month, prefix=\"month\")[col].values\n",
    "    data_grf[\"week_of_year\"] = data_grf.index.week\n",
    "    data_grf[\"day_of_month\"] = data_grf.index.day\n",
    "\n",
    "    for col in pd.get_dummies(data_grf_d.index.month, prefix=\"month\").columns:\n",
    "        data_grf_d[col] = pd.get_dummies(data_grf_d.index.month, prefix=\"month\")[col].values\n",
    "    data_grf_d[\"week_of_year\"] = data_grf_d.index.week\n",
    "    data_grf_d[\"day_of_month\"] = data_grf_d.index.day\n",
    "\n",
    "    for ci in cities:\n",
    "        if ci == city:\n",
    "            data_grf_d[ci] = [ 1 for i in range(len(data_grf_d))  ]\n",
    "        else:\n",
    "            data_grf_d[ci] = [ 0 for i in range(len(data_grf_d))  ]\n",
    "\n",
    "    X_rf = data_rf[data_rf.columns.difference(cols)]\n",
    "    X_grf = data_grf[data_grf.columns.difference(cols)]\n",
    "    X_grf_d = data_grf_d[data_grf_d.columns.difference(cols)]\n",
    "    X_var = data_var[data_var.columns.difference(cols)]\n",
    "    X_gvar = data_gvar[data_gvar.columns.difference(cols)]\n",
    "    \n",
    "    y_var = data_var[cols]\n",
    "    y_rf = data_rf[cols]\n",
    "    preds_var, preds_rf, preds_arima = list(), list(), list()\n",
    "    var_true, rf_true = list(), list()\n",
    "    var_index, rf_index = list(), list()\n",
    "    ## Rolling Pred with inital training\n",
    "    for t in range(test_size):\n",
    "        if t!= test_size-1:\n",
    "            X_train_var, y_train_var = X_var[:-test_size+t], y_var[:-test_size+t]\n",
    "            X_test_var, y_test_var = X_var[-test_size+t:-test_size+t+1], y_var[-test_size+t:-test_size+t+1]\n",
    "\n",
    "            X_train_rf, y_train_rf = X_rf[:-test_size+t], y_rf[:-test_size+t]\n",
    "            X_test_rf, y_test_rf = X_rf[-test_size+t:-test_size+t+1], y_rf[-test_size+t:-test_size+t+1]\n",
    "        else:\n",
    "            X_train_var, y_train_var = X_var[:-1], y_var[:-1]\n",
    "            X_test_var, y_test_var = X_var[-1:], y_var[-1:]\n",
    "\n",
    "            X_train_rf, y_train_rf = X_rf[:-1], y_rf[:-1]\n",
    "            X_test_rf, y_test_rf = X_rf[-1:], y_rf[-1:]\n",
    "\n",
    "        var_true.append(y_test_var.values[0])\n",
    "        var_index.append(y_test_var.index[0])\n",
    "        #print(\"Train until :\", X_train_var.index[-1], \" Test on : \", X_test_var.index[0])\n",
    "        #print(\"\\n - - - - - \")\n",
    "        rf_true.append(y_test_rf.values[0])\n",
    "        rf_index.append(y_test_rf.index[0])\n",
    "        #print(\"Train until :\",X_train_rf.index[-1], \" Test on : \", X_test_rf.index[0])\n",
    "        #print(\"\\n - - - - - \")\n",
    "\n",
    "        #Train VAR and make one step pred\n",
    "        VAR = LinearRegression()\n",
    "        VAR.fit(X_train_var, y_train_var)\n",
    "        pred_var = VAR.predict(X_test_var)\n",
    "        preds_var.append(pred_var[0])\n",
    "\n",
    "        #Train RF and make one step pred\n",
    "        RF = RandomForestRegressor(n_estimators=100,\n",
    "                                   max_depth = None, n_jobs=-1, max_features = int(X_rf.shape[1] / 3.))\n",
    "        RF.fit(X_train_rf, y_train_rf)\n",
    "        pred_rf = RF.predict(X_test_rf)\n",
    "        preds_rf.append(pred_rf[0])\n",
    "        \n",
    "        #Train ARIMA and make one step pred\n",
    "        pred_arima = pred_ARIMA(data_orig[:-test_size+t], order=(1,1,1))\n",
    "        preds_arima.append(pred_arima)\n",
    "\n",
    "    rf_true = pd.DataFrame(rf_true, index=rf_index, columns=target_vector)\n",
    "    var_true = pd.DataFrame(var_true, index=var_index, columns=target_vector)\n",
    "\n",
    "    #preds_rf = pd.DataFrame(preds_rf, index=rf_index, columns=target_vector)\n",
    "    #preds_var = pd.DataFrame(preds_var, index=var_index, columns=target_vector)\n",
    "    res = {\"True\" : pd.DataFrame(rf_true, columns=cols, index = data_orig[-test_size:].index),\n",
    "            \"BL1\" : pd.DataFrame(pred_BL1, columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"SMA\" : pd.DataFrame(pred_SMA, columns=cols, index= data_orig[-test_size:].index), \n",
    "            \"VAR\" : pd.DataFrame(preds_var, columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"RF\" : pd.DataFrame(preds_rf, columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"LWMA\" : pd.DataFrame(pred_LWMA, columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"EWMA\" : pd.DataFrame(pred_EWMA, columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"ARIMA\" : pd.DataFrame(preds_arima, columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"GRF_D\" : pd.DataFrame(grf_dummy.predict(X_grf_d[-test_size:]),\n",
    "                                   columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"GRF\" : pd.DataFrame(grf.predict(X_grf[-test_size:]),\n",
    "                                 columns=cols, index= data_orig[-test_size:].index),\n",
    "            \"GVAR\" : pd.DataFrame(gvar.predict(X_gvar[-test_size:]),\n",
    "                                columns=cols, index= data_orig[-test_size:].index)}\n",
    "\n",
    "\n",
    "    # Rescale predictions of models which used delta(x_t)\n",
    "    for x in res:\n",
    "        if x in [\"VAR\", \"GVAR\"]:\n",
    "            #rescale using x_t = delta(x_t) + x_{t-1}\n",
    "            init_loc = data_orig.index.get_loc(res[x].index[0]) - 1\n",
    "            init_val = data_orig.iloc[init_loc]\n",
    "            res[x].iloc[0] = res[x].iloc[0] + init_val\n",
    "            for i in range(1, len(res[x])):\n",
    "                res[x].iloc[i] = res[x].iloc[i] + data_orig.iloc[init_loc+i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results frequency and city wise\n",
    "res_dicts = []\n",
    "for freq in tqdm([\"1w\", \"2w\", \"3w\", \"4w\"]):\n",
    "    d = {}\n",
    "    for city in tqdm(cities):\n",
    "        res = rolling_predictions(frequency=freq, city=city)\n",
    "        RMSE_rf = np.sqrt(((res[\"True\"] - res[\"RF\"])**2).mean())\n",
    "        RMSE_grf = np.sqrt(((res[\"True\"] - res[\"GRF\"])**2).mean())\n",
    "        RMSE_grf_dummy = np.sqrt(((res[\"True\"] - res[\"GRF_D\"])**2).mean())\n",
    "        RMSE_gvar = np.sqrt(((res[\"True\"] - res[\"GVAR\"])**2).mean())\n",
    "        RMSE_var = np.sqrt(((res[\"True\"] - res[\"VAR\"])**2).mean())\n",
    "        RMSE_bl1 = np.sqrt(((res[\"True\"] - res[\"BL1\"])**2).mean())\n",
    "        RMSE_SMA = np.sqrt(((res[\"True\"] - res[\"SMA\"])**2).mean())\n",
    "        RMSE_LWMA = np.sqrt(((res[\"True\"] - res[\"LWMA\"])**2).mean())\n",
    "        RMSE_EWMA = np.sqrt(((res[\"True\"] - res[\"EWMA\"])**2).mean())\n",
    "        df = pd.DataFrame([_.to_list() for _ in [RMSE_var, RMSE_rf, RMSE_bl1, RMSE_SMA,RMSE_LWMA,RMSE_EWMA, RMSE_gvar, RMSE_grf_dummy, RMSE_grf ]],\n",
    "                                                # RMSE_grf, RMSE_grf_dummy, RMSE_gvar]],\n",
    "                               columns = target_vector,\n",
    "                               index=[\"VAR\", \"RF\", \"BL1\", \"SMA\",\"LWMA\", \"EWMA\", \"GVAR\", \"GRF_D\", \"GRF\"])\n",
    "        df.index.name = \"Model\"\n",
    "        #df[\"MVE\"] = df.mean(axis=1)\n",
    "        #df[\"SVE\"] = df.sum(axis=1)\n",
    "        d[city] = df\n",
    "    res_dicts.append(d)\n",
    "    \n",
    "# Restructure\n",
    "model_results_w = res_dicts[0]\n",
    "model_results_2w = res_dicts[1]\n",
    "model_results_3w = res_dicts[2]\n",
    "model_results_m = res_dicts[3]\n",
    "\n",
    "# Make a normalized version of the results\n",
    "model_results_w_norm = { city : model_results_w[city] / model_results_w[city].loc[\"BL1\"] for city in model_results_w  }\n",
    "model_results_2w_norm = { city : model_results_2w[city] / model_results_2w[city].loc[\"BL1\"] for city in model_results_2w  }\n",
    "model_results_3w_norm = { city : model_results_3w[city] / model_results_3w[city].loc[\"BL1\"] for city in model_results_3w  }\n",
    "model_results_m_norm = { city : model_results_m[city] / model_results_m[city].loc[\"BL1\"] for city in model_results_m  }\n",
    "\n",
    "data = aggregate(model_results_w, model_results_2w, model_results_3w, model_results_m)\n",
    "data_norm = aggregate(model_results_w_norm, model_results_2w_norm, model_results_3w_norm, model_results_m_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
